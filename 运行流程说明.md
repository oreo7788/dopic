# auto_download_comics.py 运行流程说明

## 概述

`auto_download_comics.py` 是一个自动下载漫画工具，通过多级链接提取和下载流程，从源网页自动获取并下载漫画图片。

## 完整运行流程

### 1. 初始化阶段

```
启动脚本
  ↓
解析命令行参数
  ↓
创建 ComicDownloader 实例
  ├─ 设置访问延迟 (delay)
  ├─ 设置下载延迟 (download_delay)
  ├─ 设置下载目录 (download_dir)
  ├─ 创建 requests Session
  └─ 初始化统计信息
```

**关键参数：**
- `delay`: 访问URL之间的延迟（默认: 1.0秒）
- `download_delay`: 下载图片之间的延迟（默认: 0.5秒）
- `download_dir`: 下载目录基础路径（默认: ./downloaded_images）

---

### 2. 主循环流程（支持翻页）

```
开始主循环
  ↓
检查页数限制（end_page, max_pages）
  ↓
处理当前页
  ├─ 步骤1: 获取源网页 (dnew.php)
  ├─ 步骤2: 提取 post.php 链接
  ├─ 步骤3: 提取下一页URL
  ├─ 步骤4: 处理每个 post.php 链接
  │   ├─ 访问 post.php 页面
  │   ├─ 提取 readOnline2.php 链接
  │   └─ 下载图片
  └─ 步骤5: 准备下一页
```

---

### 3. 详细步骤说明

#### 步骤1: 获取源网页 (`process_source_page`)

```
访问源网页 (dnew.php)
  ↓
获取HTML内容
  ↓
提取 post.php 链接 (extract_post_links)
  ├─ 从 <a> 标签中查找包含 "post.php" 的链接
  ├─ 提取链接中的 ID 参数
  ├─ 去重（每个ID只保留一个链接）
  └─ 按ID排序
  ↓
提取下一页URL (extract_next_page_url)
  ├─ 方法1: 查找包含"下一页"、"next"等文本的链接
  ├─ 方法2: 查找页码链接（当前页+1）
  └─ 方法3: 直接构建下一页URL
  ↓
返回 (post.php链接列表, 下一页URL)
```

**提取 post.php 链接的方法：**
- 使用 BeautifulSoup 解析HTML
- 查找所有 `<a>` 标签，筛选包含 `post.php` 的 `href` 属性
- 提取 `ID` 参数，构建完整URL
- 去重并排序

---

#### 步骤2: 处理 post.php 页面 (`process_post_page`)

```
访问 post.php?ID=xxx
  ↓
获取HTML内容
  ↓
提取 readOnline2.php 链接 (extract_readonline_links)
  ├─ 使用正则表达式匹配多种格式：
  │   ├─ 引号内的链接: "readOnline2.php?ID=xxx"
  │   ├─ href属性: href="readOnline2.php?ID=xxx"
  │   └─ 直接匹配: readOnline2.php?ID=xxx
  ├─ 排除完整URL（避免重复）
  └─ 返回相对路径集合
  ↓
构建完整URL (build_full_readonline_urls)
  ├─ 将相对路径转换为完整URL
  ├─ 去重（优先保留带host_id的链接）
  └─ 按ID排序
  ↓
返回 readOnline2.php 完整URL列表
```

**提取 readOnline2.php 链接的正则模式：**
- `['"]readOnline2\.php\?ID=(\d+)&host_id=(\d+)[^'"]*['"]`
- `['"]readOnline2\.php\?ID=(\d+)[^'"]*['"]`
- `href=['"]readOnline2\.php\?ID=(\d+)&host_id=(\d+)[^'"]*['"]`
- `href=['"]readOnline2\.php\?ID=(\d+)[^'"]*['"]`
- `readOnline2\.php\?ID=(\d+)&host_id=(\d+)`
- `readOnline2\.php\?ID=(\d+)`

---

#### 步骤3: 下载图片 (`process_readonline_page`)

```
检查下载目录是否已存在
  ├─ 如果存在 → 跳过下载，返回成功
  └─ 如果不存在 → 继续下载
  ↓
调用 download_images.py 脚本 (download_images_for_url)
  ├─ 查找脚本文件（尝试多个路径）
  ├─ 执行: python download_images.py <readOnline2.php URL>
  ├─ 等待脚本执行完成（最多10分钟超时）
  └─ 验证下载结果
      ├─ 检查返回码
      ├─ 检查文件夹是否创建
      └─ 检查文件夹中是否有文件
  ↓
更新统计信息
  ├─ 下载成功 → download_success++
  ├─ 下载跳过 → download_skipped++
  └─ 下载失败 → download_failed++
```

**下载脚本验证：**
- 从URL中提取 `ID` 参数
- 检查 `./downloaded_images/{ID}/` 目录是否存在
- 检查目录中是否有文件
- 如果目录存在且有文件，认为下载成功

---

### 4. 延迟控制

```
访问延迟 (delay)
  ├─ 在每个 post.php 链接处理之间
  └─ 在每页处理之间

下载延迟 (download_delay)
  └─ 在每个 readOnline2.php 下载之间
```

**延迟时机：**
- 处理完一个 post.php 后，等待 `delay` 秒再处理下一个
- 处理完一页后，等待 `delay` 秒再处理下一页
- 下载完一个 readOnline2.php 后，等待 `download_delay` 秒再下载下一个

---

### 5. 翻页机制

```
处理完当前页
  ↓
检查是否有下一页URL
  ├─ 有 → 继续处理下一页
  └─ 无 → 结束循环
  ↓
检查页数限制
  ├─ end_page: 如果当前页 >= end_page，停止
  └─ max_pages: 如果已处理页数 >= max_pages，停止
  ↓
更新当前页URL和页码
  ↓
继续循环
```

**翻页检测方法：**
1. 查找包含"下一页"、"next"等关键词的链接
2. 查找页码链接，匹配当前页+1
3. 如果当前URL有 `page` 参数，直接构建下一页URL
4. 如果当前URL没有 `page` 参数，添加 `page=2`

---

### 6. 统计信息

脚本会跟踪以下统计信息：

- `pages_processed`: 处理的页数
- `post_links_found`: 找到的 post.php 链接总数
- `post_links_visited`: 访问的 post.php 页面数
- `readonline_links_found`: 找到的 readOnline2.php 链接总数
- `readonline_links_visited`: 访问的 readOnline2.php 页面数
- `download_success`: 下载成功的数量
- `download_skipped`: 跳过的数量（文件夹已存在）
- `download_failed`: 下载失败的数量

---

## 完整流程图

```
开始
  ↓
初始化 ComicDownloader
  ↓
┌─────────────────────────────────┐
│ 主循环（支持翻页）                │
│                                 │
│  while True:                    │
│    ├─ 检查页数限制               │
│    ├─ 处理当前页                 │
│    │   ├─ 获取源网页             │
│    │   ├─ 提取 post.php 链接     │
│    │   ├─ 提取下一页URL          │
│    │   └─ 处理每个 post.php      │
│    │       ├─ 访问 post.php      │
│    │       ├─ 提取 readOnline2   │
│    │       └─ 下载图片           │
│    │           ├─ 检查目录存在    │
│    │           ├─ 调用下载脚本    │
│    │           └─ 验证下载结果    │
│    ├─ 检查是否有下一页           │
│    └─ 准备下一页                 │
└─────────────────────────────────┘
  ↓
打印统计信息
  ↓
结束
```

---

## 关键方法说明

### `extract_post_links(html_content, base_url)`
- **功能**: 从HTML中提取 post.php 链接
- **返回**: post.php 链接列表（包含 url, id, href）

### `extract_readonline_links(html_content)`
- **功能**: 从HTML中提取 readOnline2.php 链接
- **返回**: readOnline2.php 相对路径集合

### `build_full_readonline_urls(links)`
- **功能**: 将相对链接转换为完整URL，并去重
- **返回**: readOnline2.php 完整URL列表（已排序）

### `process_source_page(source_url)`
- **功能**: 处理源网页，提取 post.php 链接和下一页URL
- **返回**: (post.php链接列表, 下一页URL)

### `process_post_page(post_url)`
- **功能**: 处理 post.php 页面，提取 readOnline2.php 链接
- **返回**: readOnline2.php 完整URL列表

### `process_readonline_page(readonline_url, download_script)`
- **功能**: 处理 readOnline2.php 页面，下载图片
- **返回**: 是否成功（包括跳过的情况）

### `download_images_for_url(url, download_script)`
- **功能**: 调用 download_images.py 下载指定URL的图片
- **返回**: 是否成功（会验证文件夹是否真的被创建）

### `extract_next_page_url(html_content, current_url)`
- **功能**: 从HTML内容中提取下一页URL
- **返回**: 下一页URL，如果没有则返回None

---

## 命令行参数

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `-u, --url` | 源网页URL | `https://ahri8-2025-10-01-yhhmc.monster/dnew.php?category_id=1&t=1761955200` |
| `-s, --script` | 下载脚本路径 | `download_images.py` |
| `-d, --delay` | 访问URL之间的延迟（秒） | `1.0` |
| `--download-delay` | 下载图片之间的延迟（秒） | `0.5` |
| `--max-posts` | 每页最大处理post.php数量 | `None`（处理所有） |
| `--start-page` | 起始页码 | `1` |
| `--end-page` | 结束页码 | `None`（处理到最后一页） |
| `--max-pages` | 最大处理页数 | `None`（不限制） |
| `--base-url` | 基础URL | `https://ahri8-2025-10-01-yhhmc.monster/` |
| `--download-dir` | 下载目录基础路径 | `./downloaded_images` |

---

## 使用示例

### 基本使用
```bash
# 从默认源网页开始下载（处理所有页面）
python auto_download_comics.py
```

### 限制处理页数
```bash
# 只处理前3页
python auto_download_comics.py --max-pages 3
```

### 处理指定页数范围
```bash
# 处理第2页到第5页
python auto_download_comics.py --start-page 2 --end-page 5
```

### 限制每页处理数量（测试用）
```bash
# 每页只处理5个post.php链接
python auto_download_comics.py --max-posts 5
```

### 自定义延迟
```bash
# 访问延迟2秒，下载延迟1秒
python auto_download_comics.py --delay 2.0 --download-delay 1.0
```

---

## 输出目录结构

```
downloaded_images/
  ├─ {comic_id_1}/
  │   ├─ image_001.jpg
  │   ├─ image_002.jpg
  │   └─ ...
  ├─ {comic_id_2}/
  │   ├─ image_001.jpg
  │   └─ ...
  └─ ...
```

每个漫画的图片保存在以 `comic_id`（从 readOnline2.php 的 ID 参数提取）命名的文件夹中。

---

## 错误处理

1. **网络错误**: 自动记录错误日志，继续处理下一个链接
2. **脚本执行失败**: 记录错误，更新失败统计
3. **超时**: 下载脚本执行超过10分钟会超时，记录错误
4. **目录已存在**: 自动跳过，不重复下载

---

## 注意事项

1. 脚本会自动跳过已存在的下载目录，避免重复下载
2. 下载过程中会有延迟，避免请求过快
3. 所有操作都会记录日志，方便排查问题
4. 支持断点续传：如果目录已存在，会跳过该漫画的下载

